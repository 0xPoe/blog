<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>使用 TiCDC 将 TiDB 数据接入 Flink - Rustin</title>
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" type="text/css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather+Sans:400,300,700" type="text/css">
  <link rel="stylesheet" href="/static/app.css" type="text/css">
  <link rel="stylesheet" href="/static/syntax.css" type="text/css">
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
  <meta name="theme-color" content="#111111">

  <meta name="og:type" content="article">
  <meta name="og:title" content="使用 TiCDC 将 TiDB 数据接入 Flink – Rustin">
  <meta name="og:description" content="">
  <meta name="og:site_name" content="Rustin">
  <meta name="og:url" content="http://localhost:4000/%E4%BD%BF%E7%94%A8-TiCDC-%E5%B0%86-TiDB-%E6%8E%A5%E5%85%A5-Flink/">
  
  <meta name="og:image" content="http://localhost:4000/static/default_image.jpg">
  
  <meta property="article:published_time" content=" 2022-01-30">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@hi_rustin">
  <meta name="twitter:domain" content="rustin.me">
  <meta name="twitter:title" content="使用 TiCDC 将 TiDB 数据接入 Flink – Rustin">
  <meta name="twitter:description" content="">
  
  <meta name="twitter:image" content="http://localhost:4000/static/default_image.jpg">
  
  <meta name="twitter:url" content="http://localhost:4000/%E4%BD%BF%E7%94%A8-TiCDC-%E5%B0%86-TiDB-%E6%8E%A5%E5%85%A5-Flink/">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.0/anchor.min.js"></script>
</head>

<body>
  <header>
    <h1><a href="/">Rustin</a></h1>
    <span>I’m a passionate software engineer who specializes in distributed systems and dev tools.</span>
  </header>

  <div class="content post">
    <h2>使用 TiCDC 将 TiDB 数据接入 Flink</h2>
    <p class="date">30 January 2022</p>

    <blockquote>
  <p>免责声明：我既不懂 TiCDC，也不懂 Flink。所以下面的文章很有可能是一派胡言。</p>
</blockquote>

<p>最近我在帮 <a href="https://docs.pingcap.com/tidb/stable/ticdc-overview/">TiCDC</a> 的 <a href="https://docs.pingcap.com/tidb/stable/manage-ticdc#configure-sink-uri-with-kafka">Kafka Sink</a> 做多 Topic 支持，目标是让 TiCDC 能够把 <a href="https://docs.pingcap.com/tidb/stable/">TiDB</a> 的数据接入到 <a href="https://flink.apache.org/">Flink</a>。其实现在的 TiCDC 就具备接入 Flink 的能力，只是受限于目前的 Kafka Sink
只能支持单个 Topic，接入和维护非常麻烦。</p>

<p>我来介绍一下目前 TiCDC 接入 Flink 的现状和方法。</p>

<p>此博客在 <a href="https://github.com/Rustin170506/blog">GitHub</a> 上公开发布.
如果您有任何问题或疑问，请在此处打开一个 <a href="https://github.com/Rustin170506/blog/issues">issue</a>.</p>

<h2 id="简介">简介</h2>

<p>TiCDC Kafka Sink 支持多种协议格式，其中活跃维护的是 TiCDC 自己实现的 <code class="language-plaintext highlighter-rouge">open-protocol</code> 和阿里巴巴的 <code class="language-plaintext highlighter-rouge">canal-json</code> 协议。这次我们需要使用 <code class="language-plaintext highlighter-rouge">canal-json</code> 协议并通过 Flink
的 <a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/kafka/">Kafka connector</a> 接入 Flink 系统。</p>

<h2 id="启动-tidb-和-ticdc-集群">启动 TiDB 和 TiCDC 集群</h2>

<p>推荐使用 <a href="https://tiup.io/">TiUP</a> 启动 TiDB 和 TiCDC 集群，因为今天只是简单演示所以我就用 TiUP 的 Playground 启动测试集群。</p>

<p>使用以下命令启动集群：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tiup playground nightly <span class="nt">--ticdc</span> 1
</code></pre></div></div>

<p>这个命令首先会启动一个 TiDB playground 集群，其次因为我们指定了 <code class="language-plaintext highlighter-rouge">--ticdc 1</code> 所以它也会在集群中启动一个 TiCDC Server。</p>

<p>如果你在使用这个命令的过程中遇到了类似如下的错误：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error: Playground bootstrapping failed: version v5.4.0-nightly-20211225 on darwin/amd64 <span class="k">for </span>component prometheus not found: unknown version
</code></pre></div></div>

<p>你可以尝试使用 <code class="language-plaintext highlighter-rouge">tiup update playground</code> 命令更新你的 Playground 组件之后再启动。启动完成之后控制台会输出：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CLUSTER START SUCCESSFULLY, Enjoy it ^-^
To connect TiDB: mysql <span class="nt">--comments</span> <span class="nt">--host</span> 127.0.0.1 <span class="nt">--port</span> 62505 <span class="nt">-u</span> root <span class="nt">-p</span> <span class="o">(</span>no password<span class="o">)</span>
To view the dashboard: http://127.0.0.1:2379/dashboard
PD client endpoints: <span class="o">[</span>127.0.0.1:2379]
To view the Prometheus: http://127.0.0.1:9090
To view the Grafana: http://127.0.0.1:3000
</code></pre></div></div>

<p>现在我们就可以用 MySQL 客户端连接 TiDB 集群并查看版本：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mysql <span class="nt">--comments</span> <span class="nt">--host</span> 127.0.0.1 <span class="nt">--port</span> 62505 <span class="nt">-u</span> root <span class="nt">-p</span>

mysql&gt; SELECT tidb_version<span class="o">()</span><span class="p">;</span>
+-----------------------------------------------------------------------------------------------------------------------+
| tidb_version<span class="o">()</span>                                                                                                                                                                                                                                                                                                                   |
+-----------------------------------------------------------------------------------------------------------------------+
| Release Version: v5.5.0-alpha
Edition: Community
Git Commit Hash: 23f7e51ae01287fa7f811e1462d8987e4d7727a4
Git Branch: heads/refs/tags/v5.5.0-alpha
UTC Build Time: 2022-01-27 14:58:42
GoVersion: go1.16.4
Race Enabled: <span class="nb">false
</span>TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306
Check Table Before Drop: <span class="nb">false</span> |
+-----------------------------------------------------------------------------------------------------------------------+
1 row <span class="k">in </span><span class="nb">set</span> <span class="o">(</span>0.00 sec<span class="o">)</span>
</code></pre></div></div>

<p>可以看到我们启动了一个 nightly 版本的 TiDB 集群，我们使用 nightly 是因为过去几个月 TiCDC 团队对 <code class="language-plaintext highlighter-rouge">canal-json</code> 协议进行了大量的测试和问题修复，所以更推荐使用最新版本 TiCDC
的 <code class="language-plaintext highlighter-rouge">canal-json</code> 实现。 这些修复也将在 v5.4.0 发布。</p>

<p>说到 v5.4.0 ，可能大家可以看到一个奇怪的现象，上面的版本都到 v5.5.0 了，但是 v5.4.0 还没发布。这跟 TiDB 社区目前的发布和分支维护模型有关。因为我们现在已经切出了 <a href="https://github.com/pingcap/tiflow/tree/release-5.4">v5.4 分支</a>开始了发布前的测试，所以现在
master 上的 nightly 就只能用 v5.5.0-alpha 的版本标签了。</p>

<h2 id="启动-kafka-和-flink-集群">启动 Kafka 和 Flink 集群</h2>

<p>在本地直接启动 Kafka 和 Flink 集群比较繁琐且坑比较多，所以我们可以使用 docker 和 docker-compose 快速的启动 Kafka 和 Flink 集群。</p>

<p>我整理出了我在测试 Flink 时用到的 docker-compose，并将它分享到了 <a href="https://github.com/Rustin170506/ticdc-test-compose">ticdc-test-compose</a>。我们这次使用的是 Kafka connector 如果直接使用 Flink 的官方镜像，你在执行 SQL 的时候就会遇到
ClassNotFound 的 Java 异常，并且手动下载这些 Jar 包并重启 Flink 会比较麻烦，所以我自定义了一个 Dockerfile 来提前下载这些 Jar 包。</p>

<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> flink:scala_2.11-java11</span>
<span class="k">RUN </span>wget <span class="nt">-P</span> /opt/flink/lib/ https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar
<span class="k">RUN </span>wget <span class="nt">-P</span> /opt/flink/lib/ https://repo1.maven.org/maven2/org/apache/flink/flink-connector-kafka_2.11/1.14.2/flink-connector-kafka_2.11-1.14.2.jar
</code></pre></div></div>

<p>我们在原镜像的基础上，下载了 Kafka 的客户端包和 Flink Kafka 连接器的包。docker-compose 文件就不展开讲了，都是一些最基本的配置。</p>

<p>我们克隆该仓库之后在根目录通过一条命令就可以启动一个 Kafka 和 Flink 集群。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/Rustin170506/ticdc-test-compose
<span class="nb">cd </span>ticdc-test-compose
docker-compose <span class="nt">-f</span> ./docker-compose-flink.yaml up <span class="nt">-d</span>
</code></pre></div></div>

<p>启动完成之后 <code class="language-plaintext highlighter-rouge">docker ps</code>:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ docker ps
CONTAINER ID   IMAGE                            COMMAND                  CREATED          STATUS          PORTS                                  NAMES
54ccfdccb655   ticdc-test-compose_taskmanager   <span class="s2">"/docker-entrypoint.…"</span>   43 seconds ago   Up 33 seconds   6123/tcp, 8081/tcp                     ticdc-test-compose-taskmanager-1
9fb8830ff91b   ticdc-test-compose_jobmanager    <span class="s2">"/docker-entrypoint.…"</span>   44 seconds ago   Up 36 seconds   6123/tcp, 0.0.0.0:8081-&gt;8081/tcp       ticdc-test-compose-jobmanager-1
d89734157871   wurstmeister/kafka               <span class="s2">"kafka-console-consu…"</span>   44 seconds ago   Up 36 seconds                                          ticdc-test-compose-kafka-consumer-1
10b933f61756   wurstmeister/kafka               <span class="s2">"start-kafka.sh"</span>         45 seconds ago   Up 38 seconds   0.0.0.0:9092-&gt;9092/tcp                 ticdc-test-compose-kafka-1
839efd64fe11   wurstmeister/zookeeper           <span class="s2">"/bin/sh -c '/usr/sb…"</span>   46 seconds ago   Up 41 seconds   22/tcp, 2181/tcp, 2888/tcp, 3888/tcp   ticdc-test-compose-zookeeper-1
</code></pre></div></div>

<p>可以看到我们启动了一个 Kafka 集群，其中包括一个 Server 和一个 Consumer，Server 会根据配置创建一个名为 <code class="language-plaintext highlighter-rouge">ticdc-test</code> 的 Topic，Consumer 也会在 console 中开始消费这个
Topic。 另外，我们也启动了 Flink 的 jobmanager 和 taskmanager。</p>

<h2 id="创建-changefeed-开始同步数据到-kafka">创建 Changefeed 开始同步数据到 Kafka</h2>

<p>当 TiDB、Kafka 和 Flink 集群都启动之后，我们就可以使用 TiCDC 的 cli 工具创建 changefeed 进行数据同步。使用如下命令创建 changefeed：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ticdc-test-compose git:<span class="o">(</span>main<span class="o">)</span> tiup cdc:nightly cli changefeed create <span class="nt">--sink-uri</span><span class="o">=</span><span class="s2">"kafka://localhost:9092/ticdc-test?protocol=canal-json"</span>
The component <span class="sb">`</span>cdc<span class="sb">`</span> version v5.5.0-nightly-20220127 is not installed<span class="p">;</span> downloading from repository.
component cdc version v5.5.0-nightly-20220127 is already installed
Starting component <span class="sb">`</span>cdc<span class="sb">`</span>: /Users/rustin/.tiup/components/cdc/v5.5.0-nightly-20220127/cdc cli changefeed create <span class="nt">--sink-uri</span><span class="o">=</span>kafka://localhost:9092/ticdc-test?protocol<span class="o">=</span>canal-json
<span class="o">[</span>2022/01/30 14:35:11.368 +08:00] <span class="o">[</span>WARN] <span class="o">[</span>kafka.go:383] <span class="o">[</span><span class="s2">"topic's </span><span class="sb">`</span>max.message.bytes<span class="sb">`</span><span class="s2"> less than the user set </span><span class="sb">`</span>max-message-bytes<span class="sb">`</span><span class="s2">,use topic's </span><span class="sb">`</span>max.message.bytes<span class="sb">`</span><span class="s2"> to initialize the Kafka producer"</span><span class="o">]</span> <span class="o">[</span>max.message.bytes<span class="o">=</span>3145728] <span class="o">[</span>max-message-bytes<span class="o">=</span>10485760]
<span class="o">[</span>2022/01/30 14:35:11.368 +08:00] <span class="o">[</span>WARN] <span class="o">[</span>kafka.go:393] <span class="o">[</span><span class="s2">"topic already exist, TiCDC will not create the topic"</span><span class="o">]</span> <span class="o">[</span><span class="nv">topic</span><span class="o">=</span>ticdc-test] <span class="o">[</span><span class="nv">detail</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">NumPartitions</span><span class="se">\"</span><span class="s2">:1,</span><span class="se">\"</span><span class="s2">ReplicationFactor</span><span class="se">\"</span><span class="s2">:1,</span><span class="se">\"</span><span class="s2">ReplicaAssignment</span><span class="se">\"</span><span class="s2">:{</span><span class="se">\"</span><span class="s2">0</span><span class="se">\"</span><span class="s2">:[1001]},</span><span class="se">\"</span><span class="s2">ConfigEntries</span><span class="se">\"</span><span class="s2">:{</span><span class="se">\"</span><span class="s2">max.message.bytes</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">3145728</span><span class="se">\"</span><span class="s2">,</span><span class="se">\"</span><span class="s2">segment.bytes</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">1073741824</span><span class="se">\"</span><span class="s2">}}"</span><span class="o">]</span>
Create changefeed successfully!
ID: 040a4c2c-a829-42f6-b913-af9bb43c6ef4
Info: <span class="o">{</span><span class="s2">"sink-uri"</span>:<span class="s2">"kafka://localhost:9092/ticdc-test?protocol=canal-json"</span>,<span class="s2">"opts"</span>:<span class="o">{</span><span class="s2">"max-message-bytes"</span>:<span class="s2">"3145728"</span><span class="o">}</span>,<span class="s2">"create-time"</span>:<span class="s2">"2022-01-30T14:35:11.176117+08:00"</span>,<span class="s2">"start-ts"</span>:430840089420759041,<span class="s2">"target-ts"</span>:0,<span class="s2">"admin-job-type"</span>:0,<span class="s2">"sort-engine"</span>:<span class="s2">"unified"</span>,<span class="s2">"sort-dir"</span>:<span class="s2">""</span>,<span class="s2">"config"</span>:<span class="o">{</span><span class="s2">"case-sensitive"</span>:true,<span class="s2">"enable-old-value"</span>:true,<span class="s2">"force-replicate"</span>:false,<span class="s2">"check-gc-safe-point"</span>:true,<span class="s2">"filter"</span>:<span class="o">{</span><span class="s2">"rules"</span>:[<span class="s2">"*.*"</span><span class="o">]</span>,<span class="s2">"ignore-txn-start-ts"</span>:null<span class="o">}</span>,<span class="s2">"mounter"</span>:<span class="o">{</span><span class="s2">"worker-num"</span>:16<span class="o">}</span>,<span class="s2">"sink"</span>:<span class="o">{</span><span class="s2">"dispatchers"</span>:null,<span class="s2">"protocol"</span>:<span class="s2">"canal-json"</span>,<span class="s2">"column-selectors"</span>:null<span class="o">}</span>,<span class="s2">"cyclic-replication"</span>:<span class="o">{</span><span class="s2">"enable"</span>:false,<span class="s2">"replica-id"</span>:0,<span class="s2">"filter-replica-ids"</span>:null,<span class="s2">"id-buckets"</span>:0,<span class="s2">"sync-ddl"</span>:false<span class="o">}</span>,<span class="s2">"scheduler"</span>:<span class="o">{</span><span class="s2">"type"</span>:<span class="s2">"table-number"</span>,<span class="s2">"polling-time"</span>:-1<span class="o">}</span>,<span class="s2">"consistent"</span>:<span class="o">{</span><span class="s2">"level"</span>:<span class="s2">"none"</span>,<span class="s2">"max-log-size"</span>:64,<span class="s2">"flush-interval"</span>:1000,<span class="s2">"storage"</span>:<span class="s2">""</span><span class="o">}}</span>,<span class="s2">"state"</span>:<span class="s2">"normal"</span>,<span class="s2">"error"</span>:null,<span class="s2">"sync-point-enabled"</span>:false,<span class="s2">"sync-point-interval"</span>:600000000000,<span class="s2">"creator-version"</span>:<span class="s2">"v5.5.0-alpha"</span><span class="o">}</span>
</code></pre></div></div>

<p>我们成功的创建了 changefeed，但是出现了两条警告，<strong>第一条警告看起来比较诡异，因为我们并没有传递这个参数，我完了去报个 bug 修一修</strong>。第二条警告是说我们的 Topic 已经存在了，所以不会再自动创建该 Topic。</p>

<p>在 changefeed 创建成功之后，我们就可以开始同步数据了。我们首先在 TiDB <code class="language-plaintext highlighter-rouge">test</code> 数据库中创建一个 <code class="language-plaintext highlighter-rouge">simple1</code> 表并插入数据：</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">use</span> <span class="n">test</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">table</span> <span class="n">test</span><span class="p">.</span><span class="n">simple1</span>
<span class="p">(</span>
    <span class="n">id</span>  <span class="nb">int</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
    <span class="n">val</span> <span class="nb">int</span>
<span class="p">);</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">test</span><span class="p">.</span><span class="n">simple1</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
<span class="k">VALUES</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>

<p>这个时候数据已经写到 Kafka 了，我们使用 <code class="language-plaintext highlighter-rouge">docker logs ticdc-test-compose-kafka-consumer-1 -f </code> 就可以看到 Kafka console Consumer 已经输出了这两条变更:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Partition:0	{"id":0,"database":"test","table":"simple1","pkNames":null,"isDdl":true,"type":"CREATE","es":1643525212536,"ts":1643525215268,"sql":"CREATE TABLE `test`.`simple1` (`id` INT PRIMARY KEY,`val` INT)","sqlType":null,"mysqlType":null,"data":null,"old":null}
Partition:0	{"id":0,"database":"test","table":"simple1","pkNames":["id"],"isDdl":false,"type":"INSERT","es":1643525217386,"ts":1643525218526,"sql":"","sqlType":{"id":4,"val":4},"mysqlType":{"id":"int","val":"int"},"data":[{"id":"1","val":"1"}],"old":null}
</span></code></pre></div></div>

<h2 id="使用-flink-sql-建表查询">使用 Flink SQL 建表查询</h2>

<p>当数据写到 Kafka 之后，我们就可以启动 Flink SQL Client 进行建表和查询了。 首先我们需要进入 jobmanager 的容器内部：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ticdc-test-compose-jobmanager-1 bash
</code></pre></div></div>

<p>进入容器之后我们会在 flink 目录下，我们需要进入 bin 目录执行启动命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>bin
./sql-client.sh embedded
</code></pre></div></div>

<p>如果你的环境设置正确，我们现在应该就进入了 Flink SQL Client 的环境看到了小松鼠。</p>

<p>接下来执行如下的建表 SQL:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">topic_test</span>
<span class="p">(</span>
    <span class="n">id</span>  <span class="nb">int</span> <span class="k">primary</span> <span class="k">key</span><span class="p">,</span>
    <span class="n">val</span> <span class="nb">int</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
      <span class="s1">'connector'</span> <span class="o">=</span> <span class="s1">'kafka'</span><span class="p">,</span>
      <span class="s1">'topic'</span> <span class="o">=</span> <span class="s1">'ticdc-test'</span><span class="p">,</span>
      <span class="s1">'properties.bootstrap.servers'</span> <span class="o">=</span> <span class="s1">'kafka:9092'</span><span class="p">,</span>
      <span class="s1">'properties.group.id'</span> <span class="o">=</span> <span class="s1">'testGroup'</span><span class="p">,</span>
      <span class="s1">'scan.startup.mode'</span> <span class="o">=</span> <span class="s1">'earliest-offset'</span><span class="p">,</span>
      <span class="s1">'format'</span> <span class="o">=</span> <span class="s1">'canal-json'</span><span class="p">,</span>
      <span class="s1">'canal-json.ignore-parse-errors'</span> <span class="o">=</span> <span class="s1">'true'</span>
      <span class="p">);</span>
</code></pre></div></div>

<p>对于这条命令，我们需要关注 WITH 中的这些参数：</p>

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>connector</td>
      <td>指定我们使用 Kafka connector</td>
    </tr>
    <tr>
      <td>topic</td>
      <td>新建的这张表数据从 Kafka 的哪个 Topic 获取</td>
    </tr>
    <tr>
      <td>properties.bootstrap.servers</td>
      <td>Kafka Broker 的连接地址，因为我们在 docker-compose 用了 bridge 的网络，所以使用服务名加端口即可</td>
    </tr>
    <tr>
      <td>properties.group.id</td>
      <td>Kafka 消费组 ID</td>
    </tr>
    <tr>
      <td>scan.startup.mode</td>
      <td>Kafka consumer 的启动模式，从可能的最早偏移量开始</td>
    </tr>
    <tr>
      <td>format</td>
      <td>使用 <code class="language-plaintext highlighter-rouge">canal-json</code> 协议解析消息</td>
    </tr>
    <tr>
      <td>canal-json.ignore-parse-errors</td>
      <td>忽略掉解析错误，这在跳过和忽略一些不支持的 DDL 和 DML 时很有用</td>
    </tr>
  </tbody>
</table>

<p>该命令执行成功之后我们就可以查询数据了：</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="o">*</span>
<span class="k">from</span> <span class="n">topic_test</span><span class="p">;</span>
</code></pre></div></div>

<p><img src="../static/files/post-images/2022-01-30/res.jpg" alt="" height="100px" width="750px" /></p>

<p>可以看到我们前面插入的数据已经正确的被解析和查询到了。现在当你在上游插入或者删除数据时，它也会实时更新数据。</p>

<p>以上就是 TiCDC 将 TiDB 数据接入 Flink 的简单演示，但是目前由于 TiCDC 单个 changefeed 只支持同步到一个 Topic，所以在真实业务场景中可能需要创建大量的 changefeed
把每张表单独进行同步，然后在 Flink 再聚合使用。 目前 TiCDC 团队也正在实现单个 changefeed 的多 Topic 支持，希望我们能尽快的把它做完让用户更加方便的将数据接入 Flink。</p>

<h3 id="参考链接">参考链接</h3>

<p><a href="https://nightlies.apache.org/flink/flink-docs-master/zh/docs/deployment/resource-providers/standalone/docker/">Flink Docker Setup</a></p>

<p><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/canal/">Flink Canal</a></p>



    <p class="signature">&mdash; Rustin</p>
  </div>
  <script>
    (function () {
      anchors.options.placement = 'right';
      anchors.add('.content > h3, .content > h4, .content > h5, .content > h6');
    })();
  </script>
  <script src="https://giscus.app/client.js" data-repo="Rustin170506/blog"
    data-repo-id="MDEwOlJlcG9zaXRvcnkyMTU4MDI0NDg=" data-category="Ideas" data-category-id="DIC_kwDODNziUM4CY_bg"
    data-mapping="title" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top"
    data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async>
    </script>
</body>

</html>
